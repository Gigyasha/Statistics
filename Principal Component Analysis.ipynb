{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PRINCIPAL COMPONENT ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why PCA is used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA is used to reduce dimension.<br>\n",
    "you must be asking why do we need to reduce dimesion and what exactly dimensionality reduction is?<br>\n",
    "solution:-->  When we are dealing with a dataset that contains large amount of features than its not necessary that everytime all the features are useful sometimes our dataset conatins features which are not that useful infact they are not helpful or playing a part in classifying the data/or they are not affecting the classification much, so its necessary to remove such features from our data set.<br>\n",
    "But why do we need to remove these features?<br>\n",
    "Its because these type of features are those which are correlated to some another feature means there exists a correlation between our features that why removing such feature do not affects our data , when we have correlated feature in our dataset than the covariance between such features gives high value and beacuse of that the covariance matrix came out as singular and if the covariance matrix is singular then we are unable to find inverse of covariance matrix and so we cant apply Naive bayes <br>\n",
    "That's why we reduce the dimension(feature) of our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\begin{bmatrix}\n",
    "    [ \\ldots & 1 \\times N & \\ldots  ] \\\\ \n",
    "    [\\ldots & 1 \\times N &\\ldots  ] \\\\\n",
    "     \\ldots &  \\vert &\\ldots \\\\\n",
    "    [\\ldots & 1 \\times N &\\ldots  ] \\\\\n",
    " \\end{bmatrix} \\underset{P\\times N} \\;\\; \\Rightarrow\n",
    " \\begin{bmatrix}\n",
    "   [ \\ldots & 1 \\times P & \\ldots  ] \\\\ \n",
    "    [\\ldots & 1 \\times P &\\ldots  ] \\\\\n",
    "     \\ldots &  \\vert &\\ldots \\\\\n",
    "    [\\ldots & 1 \\times P &\\ldots  ] \\\\\n",
    " \\end{bmatrix}\\underset{P\\times L}\\\\ \\;\\;\\;\\;\\ \\text{we can see here the rows p remains the same after reduction but the columns are reduced from N to L}\n",
    "\\end{equation} \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{equation}\n",
    " \\begin{bmatrix}\n",
    "    1 \\times N    \\\\ \n",
    "   \\end{bmatrix} \\;\\; \\text{(original)} \\;\\; \\Rightarrow\n",
    " \\begin{bmatrix}\n",
    "  1 \\times L    \\\\ \n",
    " \\end{bmatrix} \\;\\; \\text{(reduced) where the N dimension has been reduced to L} \\;\\; \\Rightarrow\n",
    " \\begin{bmatrix}\n",
    "    1 \\times N    \\\\ \n",
    "   \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " X^{'i}\\;\\;\\; \\;\\; \\Rightarrow \\;\\;\\;\\;\\; C^{i} \\;\\;\\; \\;\\; \\Rightarrow R^{i} \\\\\n",
    " \\text{here }\\; X^{'i}\\; \\text{represents any ith row vector or feature vector }\\\\\n",
    " \\text{here }\\; C^{'i}\\; \\text{represents the coded or compressed vector }\\\\\n",
    " \\text{here }\\; R^{'i}\\; \\text{represents Reconstructed vector which is reconstructed using coded vector }\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    " \\begin{bmatrix}\n",
    "    1 \\times N    \\\\ \n",
    "   \\end{bmatrix} \\;\\; \\Rightarrow\\;\\; \\text{Compression} \\;\\; \\Rightarrow\n",
    " \\begin{bmatrix}\n",
    "  1 \\times L    \\\\ \n",
    " \\end{bmatrix}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{bmatrix}\n",
    "  1 \\times L    \\\\ \n",
    " \\end{bmatrix} \\;\\; \\Rightarrow\\;\\; \\text{Reconstruction} \\;\\; \\Rightarrow\n",
    " \\begin{bmatrix}\n",
    "    1 \\times N    \\\\ \n",
    "   \\end{bmatrix}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<img src=\"20200627_222732.JPG\" width=\"500\" height=\"100\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See from the Diagram above we are trying to convert original feature vector to coded vector<br>\n",
    "__Let's first focus on Decoding part__ <br>\n",
    "Hoe can we go from C_i to R_i or from 1\\*L to 1\\*N we are transforming the dimension it means we need to multiply C_i with a matrix of dimension L\\*N lets take M as that Matrix so,<br>\n",
    "\\begin{equation}\n",
    "\\underset{1\\times N}R^{i}=\\underset{1\\times L}C^{i}.\\underset{L\\times N}M \\;\\;\\;\\; (1)\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are doing is for any i'th row (any one) we are trying to convert its dimension from 1\\*N to 1\\*L here N is original columns or features are L defined Reduced Columns that process is called Compression or encoding.<br>\n",
    "The Process of Reconstructing back the original vector from the coded vector is called decompression or decoding.Our main motive is to find such coded vector c_i so that it stores the maximum amount of data so that when we try to reconstruct our original vector R_i from C_i so the R_i must be ad similar as the original vector X_i.<br>\n",
    "it means the difference between the original vector(X_i) and the Recontruction vector( R_i) should be minumum.<br>\n",
    "How to find the difference between two vectors ( the magnitude/L2 norm of difference between the vectors)<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Reconstruction Error} = || X^{'i} - R^{'i} || _{2} \\;\\; \\\\\n",
    "\\text{L2 norm or magnitude of difference between original and reconstructed}\n",
    "\\end{equation}\n",
    "\n",
    "We want to miniminze it , it means the difference between the Reconstruced and orignal is very less such that Reconstructed is almost similar to original it means that the coded vectors store the maximum important information<br>\n",
    "but Reconstruction error is a non convex function so finding its minima is going to be tough to make it easier we need to convert it in convex form by squaring the reconstruction error<br>\n",
    "\n",
    "\\begin{equation}\n",
    "\\text{Square Reconstruction Error} = || X^{'i} - R^{'i} ||^{2} _{2} \\;\\; \\\\\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for some time consider that from n columns we are Reducing it to 1 single most important coloumn than the quation 1 will be<br>\n",
    "\n",
    "\\begin{equation} \\;\\; \\text{L=1,}\\;\\;\n",
    "\\underset{1\\times N}R^{i}=\\underset{1\\times 1}C^{i}.\\underset{1\\times N}M \\;\\;\\;\\;\\; \\text{as } C^{i} \\text{ is 1*1 it means its a scalar }\n",
    "\\end{equation}\n",
    "\n",
    "We want to find that value of coded vector(c_i) so that our square reconstruction gives the minimum value.\n",
    "\n",
    "\\begin{equation} \n",
    "\\text{SQUARE RECONSTRUCTION ERROR}=f(C^{i})=|| X^{'i} - R^{'i} ||^{2} _{2} \\;\\; \\text{both}\\;X^{'i} \\;and\\;R^{'i} \\; \\text{are vectors of dimension}\\; 1\\times N\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we also write square os L2 norm as $$|| \\vec{X} ||^{2}_{2} = [ \\text{row vector of X} ][ \\text{column vector of X}] $$\n",
    "as ||X-R|| is already a row vector so breaking square reconstruction error we get\n",
    "$$f(C^{i}) = (X^{'i}-R^{i})(X^{'i}-R^{i})^{T}$$\n",
    "$$f(C^{i}) = (X^{'i}-R^{i})(X^{'iT}-R^{iT})$$\n",
    "$$f(C^{i}) = (X^{'i} X^{'iT}-X^{'i}R^{iT}-R^{i} X^{'iT}+R^{i}R^{iT})$$\n",
    "Solving the insides of f(C)\n",
    "\\begin{equation}\n",
    "X^{'i} X^{'iT} \\implies||X^{'i}||^{2}_{2}\\\\\n",
    "X^{'i}R^{iT}=X^{'i}(C^{i}M)^{T} \\;\\; and \\;\\; R^{i} X^{'iT}=(C^{i}M)X^{'iT} \\;\\; and\\;\\; R^{i}R^{iT}=(C^{i}M)(C^{i}M)^{T}\\\\\n",
    "\\;\\; as\\;\\; R^{i}=C^{i}M \\;\\; \\text{from equation 1}\\\\\n",
    "\\text{putting all these values in } \\;\\; f(C^{i})\\\\\n",
    "\\text{we get}\n",
    "\\end{equation}<br>\n",
    "\n",
    "\\begin{equation}\n",
    "f(C^{i}) = ||X^{'i}||^{2}_{2}-X^{'i}M^{T}C^{iT}-C^{i}MX^{'iT}+C^{i}MM^{T}.C^{iT}\n",
    "\\end{equation}\n",
    "$$f(C^{i}) = ||X^{'i}||^{2}_{2}-X^{'i}M^{T}C^{iT}-C^{i}MX^{'iT}+C^{i}.||M||^{2}_{2}C^{iT}$$\n",
    "$$f(C^{i}) = ||X^{'i}||^{2}_{2}-X^{'i}M^{T}C^{iT}-C^{i}MX^{'iT}+C^{i2}.||M||^{2}_{2}$$\n",
    "$$f(C^{i}) = ||X^{'i}||^{2}_{2}-2C^{i}X^{'i}M^{T}+C^{i2}.||M||^{2}_{2}\\\\ \\text{same}$$\n",
    "$$f(C^{i}) = ||X^{'i}||^{2}_{2}-2C^{i}MX^{'iT}+C^{i2}.||M||^{2}_{2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function has square in it meand degree 2 such functions are convex functions are to find the value of c_i we need to find derivate with respect to c_i (M ,X constants)\n",
    "on calculating the derivate we get<br>\n",
    "$$C^{i}=X^{i}M^{T}$$\n",
    "$$ \\text{Sq Reconstruction Error} = || X^{'i} - R^{'i} ||^{2} _{2} \\;\\; \\text{this is for one ith example and this is square of magnitude so scalar} $$\n",
    "Square reconstruction for all P examples<br>\n",
    "$$|| X^{'0} - R^{'0} ||^{2} _{2}+|| X^{'1} - R^{'1} ||^{2} \\;\\;......\\;\\; || X^{'(P-1)} - R^{'(P-1)} ||^{2} _{2}$$\n",
    "__So Average square reconstruction Error__\n",
    "$$\\frac{1}{p}\\sum_{i=0}^{(P-1)}|| X^{'i} - R^{'i} ||^{2} _{2}\\;\\;\\;\\;(1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing some side calculations which we are going to put in equation 1<br>\n",
    "\\begin{equation}\n",
    "(X^{'}-R)^{T} = \\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert &\\vert \\\\\n",
    "    (X^{'0}-R^{0})^{T}  & (X^{'1}-R^{1})^{T} & \\ldots & (X^{'(P-1)}-R^{(P-1)})^{T}\\\\\n",
    "    \\vert & \\vert & \\vert & \\vert\n",
    "\\end{bmatrix}\n",
    " \\end{equation} \n",
    "\\begin{equation}\n",
    "(X^{'}-R) = \\begin{bmatrix}\n",
    "    \\vert & \\vert & \\vert &\\vert \\\\\n",
    "    (X^{'0}-R^{0})  & (X^{'1}-R^{1}) & \\ldots & (X^{'(P-1)}-R^{(P-1)})\\\\\n",
    "    \\vert & \\vert & \\vert & \\vert\n",
    "\\end{bmatrix}\n",
    " \\end{equation} <br>\n",
    " \n",
    " Multiplying both matrix and Taking their trace we get<br>\n",
    " $$Tr((X^{'}-R)(X^{'}-R)^{T})=\\sum_{i=0}^{(P-1)}|| X^{'i} - R^{'i} ||^{2} _{2}\\;\\;\\;\\;(2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puting the value of equation 2 in 1 we get,<br>\n",
    "$$\\text{Average Sq Reconstruction Error (ASRE)}=Tr((X^{'}-R)(X^{'}-R)^{T})$$\n",
    "$$\\rightarrow \\frac{1}{P}Tr(X^{'}X^{'T}-X^{'}R^{T}-RX^{'}-R.R^{T})$$\n",
    "transvering trace inside\n",
    "$$\\rightarrow \\frac{1}{P} [Tr(X^{'}X^{'T})-Tr(X^{'}R^{T})-Tr(RX^{'T})+Tr(R.R^{T})]\\;\\;\\;\\;  (3)$$\n",
    "$$\\rightarrow Tr(X^{'}X^{'T})=||X^{'i}||^{2}_{2}\\;\\;\\;\\;  (4)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "putting these values in equation 3 we get,\n",
    "$$\\rightarrow \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr(X^{'}R^{T})-Tr(RX^{'T})+Tr(R.R^{T}) \\right] \\;\\;\\;\\;  $$<br>\n",
    "$$X^{'}R^{T}\t\\leftrightarrow RX^{'T} \\;\\; \\text{both are similar and have same dimensions as}\\;\\; P\\times P \\;\\;so$$\n",
    "\n",
    "$$\\rightarrow \\boxed{ \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-2Tr(X^{'}R^{T})+Tr(R.R^{T}) \\right] } \\;\\;\\;\\;  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Performing some side calculation \n",
    "\\begin{equation}\\;\\; \\text{as we already knew from above equations that }\\;\n",
    "R=CM \\;\\; \\text{and}\\;\\;\n",
    "C=\\begin{bmatrix}\n",
    "c^{0}\\\\\n",
    "c^{1}\\\\\n",
    " \\vert\\\\\n",
    "c^{(P-1)}\\\\\n",
    "\\end{bmatrix}\\;\\; P\\times1\\;\\;{and}\\;\\;\n",
    "M=\\begin{bmatrix}\n",
    " m^{0}&m^{1} & \\ldots & m^{(P-1)}\n",
    " \\end{bmatrix}\\;\\; 1\\times P\\;\\;\n",
    "\\end{equation}\n",
    "Multiplying these two we get\n",
    "\\begin{equation}\n",
    " \\begin{bmatrix}\n",
    "    [ \\ldots & X^{'0}M^{T} & \\ldots  ] \\\\ \n",
    "    [\\ldots & X^{'1}M^{T} &\\ldots  ] \\\\\n",
    "     \\ldots &  \\vert &\\ldots \\\\\n",
    "    [\\ldots & X^{'(P-1)}M^{T} &\\ldots  ] \\\\\n",
    " \\end{bmatrix} \\underset{P\\times N} \\;\\; \\Rightarrow\\;\\; C=X^{'}M^{T}\\;\\; {and}\\;\\; R=CM \\;\\;{so}\\\\\n",
    " R=X^{'}M^{T}M\\;\\;\\; {and}\\;\\;\\;R^{T}=M^{T}MX^{'T}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting the values  of R and R_Transpose in the block equation we get\n",
    "$$\\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-2Tr(X^{'}M^{T}MX^{'T})+Tr(X^{'}M^{T}M.M^{T}MX^{'T}) \\right] \\;\\;\\;\\; $$\n",
    "Solving it a bit we get\n",
    "\n",
    "$$\\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-2||C||^{2}_{2}+||C||^{2}_{2} \\right] \\;\\;\\;\\; $$<br>\n",
    "\n",
    "$$\\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-||C||^{2}_{2} \\right] \\;\\;{as}\\;\\; ||C||^{2}_{2}=Tr((X^{'}M^{T})(MX^{'T})) \t\\leftrightarrow \\; Tr(C.C^{T})\\;\\;\\text{order of both can be changed both order will give resultant as magnitude so,}\\\\\n",
    "\\Rightarrow \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr((X^{'}M^{T})(MX^{'T}))\\right] \\;\\;\\;\\; $$\n",
    "$$\\Rightarrow \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr((MX^{'T})(X^{'}M^{T}))\\right] \\;\\;Tr(CC^{T})=Tr(C^{T}C)\\;\\;\\text{and trace (scalar) = scalar} $$\n",
    "\n",
    "$$ ASRE=\\boxed{ \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr((MX^{'T})(X^{'}M^{T}))\\right]} \\;\\;\\;\\; $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have calculated the minimum value of C which gives ASRE but than conatins M now we need to find Such M that our ASRE equation gives the minium value\n",
    "$$  \\min_{M}  \\frac{1}{P} \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr((MX^{'T})(X^{'}M^{T}))\\right] $$\n",
    "Transvering P inside\n",
    "$$  \\min_{M}   \\left[  \\sum_{i=0}^{(P-1)}||X^{'i}||^{2}_{2}-Tr((M\\frac{1}{P}(X^{'T}X^{'})M^{T})) \\right] $$\n",
    "$$ASRE(M)=f(M)=\\frac{1}{P}(X^{'T}X^{'}) = \\min_{M}   \\left[  -M\\Sigma M^{T} \\right]$$\n",
    "$$ASRE(M)=f(M) = \\max_{M}   \\left[  M\\Sigma M^{T} \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
